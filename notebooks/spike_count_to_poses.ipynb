{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ba9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57ba6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import motorlab as ml\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fb9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path().resolve().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd22334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(experiment=\"gbyk\"):\n",
    "    if experiment == \"gbyk\":\n",
    "        sessions = [\n",
    "            # \"bex_20230621_spikes_sorted_SES\",  # before\n",
    "            # \"bex_20230624_spikes_sorted_SES\",  # before\n",
    "            # \"bex_20230629_spikes_sorted_SES\",  # before\n",
    "            # \"bex_20230630_spikes_sorted_SES\",  # before\n",
    "            # \"bex_20230701_spikes_sorted_SES\",  # before\n",
    "            # \"bex_20230708_spikes_sorted_SES\",  # while\n",
    "            # \"ken_20230614_spikes_sorted_SES\",  # while and before\n",
    "            \"ken_20230618_spikes_sorted_SES\",  # before\n",
    "            # \"ken_20230622_spikes_sorted_SES\",  # while, before and free\n",
    "            # \"ken_20230629_spikes_sorted_SES\",  # while, before and free\n",
    "            # \"ken_20230630_spikes_sorted_SES\",  # while\n",
    "            # \"ken_20230701_spikes_sorted_SES\",  # before\n",
    "            # \"ken_20230703_spikes_sorted_SES\",  # while\n",
    "        ]\n",
    "    else:\n",
    "        sessions = [\n",
    "            \"bex_20230221\",\n",
    "            \"bex_20230222\",\n",
    "            \"bex_20230223\",\n",
    "            \"bex_20230224\",\n",
    "            \"bex_20230225\",\n",
    "            \"bex_20230226\",\n",
    "            \"jon_20230125\",\n",
    "            \"jon_20230126\",\n",
    "            \"jon_20230127\",\n",
    "            \"jon_20230130\",\n",
    "            \"jon_20230131\",\n",
    "            \"jon_20230202\",\n",
    "            \"jon_20230203\",\n",
    "            \"luk_20230126\",\n",
    "            \"luk_20230127\",\n",
    "            \"luk_20230130\",\n",
    "            \"luk_20230131\",\n",
    "            \"luk_20230202\",\n",
    "            \"luk_20230203\",\n",
    "        ]\n",
    "\n",
    "    config = {\n",
    "        \"data_dir\": f\"data/{experiment}\",\n",
    "        \"checkpoint_dir\": \"checkpoint/spike_count_to_pose\",\n",
    "        \"config_dir\": \"config/spike_count_to_pose\",\n",
    "        \"save\": False,\n",
    "        \"experiment\": experiment,\n",
    "        \"seed\": 50,\n",
    "        \"homing\": False,\n",
    "        \"filter\": False,\n",
    "        \"in_modalities\": \"spike_count\",\n",
    "        \"out_modalities\": \"poses\",\n",
    "        \"architecture\": \"gru\",\n",
    "        \"sessions\": sessions,\n",
    "        \"body_repr\": \"egocentric\",\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"metric\": \"correlation\",\n",
    "        \"model\": {\n",
    "            \"embedding_dim\": 512,\n",
    "            \"hidden_dim\": 512,\n",
    "            \"n_layers\": 1,\n",
    "            \"readout\": \"linear\",\n",
    "            \"dropout\": 0,\n",
    "        },\n",
    "        \"train\": {\"n_epochs\": 500, \"lr\": 3e-4},\n",
    "        \"track\": {\"metrics\": True, \"wandb\": False},\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a95dca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (in_layer): LinearEmbedding(\n",
      "    (embedding): ModuleDict(\n",
      "      (ken_20230618_spikes_sorted_SES): Linear(in_features=390, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (core): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "  (out_layer): LinearReadout(\n",
      "    (readout): ModuleDict(\n",
      "      (ken_20230618_spikes_sorted_SES): Linear(in_features=1024, out_features=63, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch: 0000 | loss: 0.0402 | grad_norm: 0.44424082 | local correlation: 0.0566 | global correlation: 0.3229\n",
      "epoch: 0001 | loss: 0.0096 | grad_norm: 0.16411289 | local correlation: 0.1267 | global correlation: 0.5279\n",
      "epoch: 0002 | loss: 0.0067 | grad_norm: 0.12990822 | local correlation: 0.1707 | global correlation: 0.5776\n",
      "epoch: 0003 | loss: 0.0057 | grad_norm: 0.11128722 | local correlation: 0.1960 | global correlation: 0.6031\n",
      "epoch: 0004 | loss: 0.0051 | grad_norm: 0.10773832 | local correlation: 0.2131 | global correlation: 0.6161\n",
      "epoch: 0005 | loss: 0.0047 | grad_norm: 0.14268294 | local correlation: 0.2278 | global correlation: 0.6285\n",
      "epoch: 0006 | loss: 0.0044 | grad_norm: 0.13410646 | local correlation: 0.2390 | global correlation: 0.6362\n",
      "epoch: 0007 | loss: 0.0043 | grad_norm: 0.14877597 | local correlation: 0.2471 | global correlation: 0.6425\n",
      "epoch: 0008 | loss: 0.0040 | grad_norm: 0.13662408 | local correlation: 0.2541 | global correlation: 0.6481\n",
      "epoch: 0009 | loss: 0.0039 | grad_norm: 0.13569570 | local correlation: 0.2599 | global correlation: 0.6545\n",
      "epoch: 0010 | loss: 0.0037 | grad_norm: 0.14398944 | local correlation: 0.2655 | global correlation: 0.6607\n",
      "epoch: 0011 | loss: 0.0036 | grad_norm: 0.09514585 | local correlation: 0.2690 | global correlation: 0.6640\n",
      "epoch: 0012 | loss: 0.0035 | grad_norm: 0.07852851 | local correlation: 0.2726 | global correlation: 0.6665\n",
      "epoch: 0013 | loss: 0.0034 | grad_norm: 0.08137059 | local correlation: 0.2746 | global correlation: 0.6686\n",
      "epoch: 0014 | loss: 0.0033 | grad_norm: 0.08014470 | local correlation: 0.2772 | global correlation: 0.6725\n",
      "epoch: 0015 | loss: 0.0033 | grad_norm: 0.07970893 | local correlation: 0.2786 | global correlation: 0.6739\n",
      "epoch: 0016 | loss: 0.0033 | grad_norm: 0.11742523 | local correlation: 0.2794 | global correlation: 0.6754\n",
      "epoch: 0017 | loss: 0.0033 | grad_norm: 0.08907372 | local correlation: 0.2803 | global correlation: 0.6757\n",
      "epoch: 0018 | loss: 0.0032 | grad_norm: 0.07902434 | local correlation: 0.2807 | global correlation: 0.6765\n",
      "epoch: 0019 | loss: 0.0032 | grad_norm: 0.09130609 | local correlation: 0.2809 | global correlation: 0.6770\n",
      "local correlation: 0.2931 | global correlation: 0.6411\n",
      "epoch: 0020 | loss: 0.0032 | grad_norm: 0.06187875 | local correlation: 0.2810 | global correlation: 0.6770\n",
      "epoch: 0021 | loss: 0.0032 | grad_norm: 0.07411834 | local correlation: 0.2810 | global correlation: 0.6770\n",
      "epoch: 0022 | loss: 0.0032 | grad_norm: 0.06664084 | local correlation: 0.2810 | global correlation: 0.6769\n",
      "epoch: 0023 | loss: 0.0032 | grad_norm: 0.07821067 | local correlation: 0.2813 | global correlation: 0.6768\n",
      "epoch: 0024 | loss: 0.0032 | grad_norm: 0.05274352 | local correlation: 0.2818 | global correlation: 0.6770\n",
      "epoch: 0025 | loss: 0.0032 | grad_norm: 0.07454159 | local correlation: 0.2829 | global correlation: 0.6767\n",
      "epoch: 0026 | loss: 0.0032 | grad_norm: 0.08098074 | local correlation: 0.2840 | global correlation: 0.6770\n",
      "epoch: 0027 | loss: 0.0032 | grad_norm: 0.07838884 | local correlation: 0.2855 | global correlation: 0.6778\n",
      "epoch: 0028 | loss: 0.0031 | grad_norm: 0.09438756 | local correlation: 0.2886 | global correlation: 0.6806\n",
      "epoch: 0029 | loss: 0.0031 | grad_norm: 0.11651803 | local correlation: 0.2907 | global correlation: 0.6791\n",
      "epoch: 0030 | loss: 0.0031 | grad_norm: 0.17432782 | local correlation: 0.2936 | global correlation: 0.6797\n",
      "epoch: 0031 | loss: 0.0031 | grad_norm: 0.14608645 | local correlation: 0.2970 | global correlation: 0.6816\n",
      "epoch: 0032 | loss: 0.0030 | grad_norm: 0.12489237 | local correlation: 0.3012 | global correlation: 0.6833\n",
      "epoch: 0033 | loss: 0.0030 | grad_norm: 0.09107521 | local correlation: 0.3034 | global correlation: 0.6838\n",
      "epoch: 0034 | loss: 0.0029 | grad_norm: 0.15885284 | local correlation: 0.3074 | global correlation: 0.6869\n",
      "epoch: 0035 | loss: 0.0030 | grad_norm: 0.12263984 | local correlation: 0.3114 | global correlation: 0.6842\n",
      "epoch: 0036 | loss: 0.0027 | grad_norm: 0.13822284 | local correlation: 0.3149 | global correlation: 0.6940\n",
      "epoch: 0037 | loss: 0.0026 | grad_norm: 0.16122568 | local correlation: 0.3181 | global correlation: 0.6964\n",
      "epoch: 0038 | loss: 0.0027 | grad_norm: 0.07954310 | local correlation: 0.3204 | global correlation: 0.6992\n",
      "epoch: 0039 | loss: 0.0026 | grad_norm: 0.12201365 | local correlation: 0.3256 | global correlation: 0.7011\n",
      "local correlation: 0.3360 | global correlation: 0.6685\n",
      "epoch: 0040 | loss: 0.0024 | grad_norm: 0.14840183 | local correlation: 0.3286 | global correlation: 0.7070\n",
      "epoch: 0041 | loss: 0.0024 | grad_norm: 0.18396547 | local correlation: 0.3325 | global correlation: 0.7060\n",
      "epoch: 0042 | loss: 0.0023 | grad_norm: 0.17321112 | local correlation: 0.3350 | global correlation: 0.7105\n",
      "epoch: 0043 | loss: 0.0022 | grad_norm: 0.12782695 | local correlation: 0.3389 | global correlation: 0.7125\n",
      "epoch: 0044 | loss: 0.0021 | grad_norm: 0.15591281 | local correlation: 0.3422 | global correlation: 0.7170\n",
      "epoch: 0045 | loss: 0.0020 | grad_norm: 0.13983577 | local correlation: 0.3456 | global correlation: 0.7204\n",
      "epoch: 0046 | loss: 0.0020 | grad_norm: 0.10162558 | local correlation: 0.3469 | global correlation: 0.7219\n",
      "epoch: 0047 | loss: 0.0019 | grad_norm: 0.11312057 | local correlation: 0.3502 | global correlation: 0.7263\n",
      "epoch: 0048 | loss: 0.0018 | grad_norm: 0.05851352 | local correlation: 0.3527 | global correlation: 0.7275\n",
      "epoch: 0049 | loss: 0.0018 | grad_norm: 0.09845504 | local correlation: 0.3543 | global correlation: 0.7302\n",
      "epoch: 0050 | loss: 0.0018 | grad_norm: 0.09313801 | local correlation: 0.3561 | global correlation: 0.7312\n",
      "epoch: 0051 | loss: 0.0017 | grad_norm: 0.06714480 | local correlation: 0.3578 | global correlation: 0.7330\n",
      "epoch: 0052 | loss: 0.0017 | grad_norm: 0.08889701 | local correlation: 0.3593 | global correlation: 0.7351\n",
      "epoch: 0053 | loss: 0.0016 | grad_norm: 0.07770777 | local correlation: 0.3598 | global correlation: 0.7361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/motorlab/model.py:265\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    256\u001b[39m optimizer = torch.optim.Adam(\n\u001b[32m    257\u001b[39m     model.parameters(),\n\u001b[32m    258\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3e-4\u001b[39m),\n\u001b[32m    259\u001b[39m     weight_decay=config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n\u001b[32m    260\u001b[39m )\n\u001b[32m    261\u001b[39m scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n\u001b[32m    262\u001b[39m     optimizer, config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mT_max\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m20\u001b[39m)\n\u001b[32m    263\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_modalities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min_modalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_modalities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mout_modalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.get(\u001b[33m\"\u001b[39m\u001b[33msave\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    278\u001b[39m     save_model(model, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/motorlab/model.py:120\u001b[39m, in \u001b[36mloop\u001b[39m\u001b[34m(model, train_dataloaders, valid_dataloaders, in_modalities, out_modalities, loss_fns, optimizer, scheduler, config)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloop\u001b[39m(\n\u001b[32m    109\u001b[39m     model,\n\u001b[32m    110\u001b[39m     train_dataloaders,\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m     config,\n\u001b[32m    118\u001b[39m ):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mn_epochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         train_metrics, _, _ = \u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m            \u001b[49m\u001b[43min_modalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mout_modalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloss_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m         train_metrics[\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m] = i\n\u001b[32m    132\u001b[39m         track(train_metrics, config[\u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/motorlab/model.py:79\u001b[39m, in \u001b[36miterate\u001b[39m\u001b[34m(model, dataloaders, in_modalities, out_modalities, loss_fns, optimizer, scheduler, mode, config)\u001b[39m\n\u001b[32m     77\u001b[39m optimizer.zero_grad()\n\u001b[32m     78\u001b[39m loss = loss_fns[session](pred, y)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m optimizer.step()\n\u001b[32m     81\u001b[39m losses.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/.mlab/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/.mlab/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/motorlab/.mlab/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ml.model.train(get_config())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
