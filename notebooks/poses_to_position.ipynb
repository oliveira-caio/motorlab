{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3e2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07068ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import motorlab as ml\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b3eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path().resolve().parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9641d",
   "metadata": {},
   "source": [
    "### pcs to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cb9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained in the notebook 'analysis_pca.ipynb'\n",
    "\n",
    "# delete pcs that are >= 0.15 above baseline.\n",
    "with open(\"artifacts/tables/analysis_pca/loose.yml\", \"r\") as f:\n",
    "    loose = yaml.safe_load(f)\n",
    "\n",
    "# delete pcs that are >= 0.10 above baseline.\n",
    "with open(\"artifacts/tables/analysis_pca/medium.yml\", \"r\") as f:\n",
    "    medium = yaml.safe_load(f)\n",
    "\n",
    "# delete pcs that are >= 0.05 above baseline.\n",
    "with open(\"artifacts/tables/analysis_pca/strict.yml\", \"r\") as f:\n",
    "    strict = yaml.safe_load(f)\n",
    "\n",
    "# delete pcs that are >= 0.01 above baseline.\n",
    "with open(\"artifacts/tables/analysis_pca/draconian.yml\", \"r\") as f:\n",
    "    draconian = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac273eda",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809c7281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 205,336\n",
      "FCModel(\n",
      "  (embedding): LinearEmbedding(\n",
      "    (linear): ModuleDict(\n",
      "      (bex_20230621_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (bex_20230624_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (bex_20230629_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (bex_20230630_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (bex_20230701_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (bex_20230708_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230618_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230622_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230629_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230630_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230701_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "      (ken_20230703_spikes_sorted_SES): ModuleDict(\n",
      "        (poses): Linear(in_features=21, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (readout): Readout(\n",
      "    (readouts): ModuleDict(\n",
      "      (bex_20230621_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (bex_20230624_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (bex_20230629_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (bex_20230630_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (bex_20230701_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (bex_20230708_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230618_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230622_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230629_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230630_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230701_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "      (ken_20230703_spikes_sorted_SES): ModuleDict(\n",
      "        (position): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "uid: 20250729195943\n",
      "epoch: 0001 | mse: 6.0717\n",
      "epoch: 0001 | grad_norm: 7.23096609 | loss: 2.3632 | mse: 2.4143\n",
      "epoch: 0002 | grad_norm: 1.28166199 | loss: 1.0681 | mse: 1.0888\n",
      "epoch: 0003 | grad_norm: 2.99839234 | loss: 0.8004 | mse: 0.8350\n",
      "epoch: 0004 | grad_norm: 1.90272141 | loss: 0.7024 | mse: 0.7216\n",
      "epoch: 0005 | grad_norm: 0.91993171 | loss: 0.6970 | mse: 0.7073\n",
      "epoch: 0006 | grad_norm: 0.74537271 | loss: 0.5994 | mse: 0.6195\n",
      "epoch: 0007 | grad_norm: 0.61071545 | loss: 0.6158 | mse: 0.6312\n",
      "epoch: 0008 | grad_norm: 1.76148093 | loss: 0.5826 | mse: 0.6048\n",
      "epoch: 0009 | grad_norm: 2.19614792 | loss: 0.5912 | mse: 0.6148\n",
      "epoch: 0010 | grad_norm: 1.50774646 | loss: 0.6108 | mse: 0.6311\n",
      "epoch: 0011 | grad_norm: 1.22834873 | loss: 0.5654 | mse: 0.5887\n",
      "epoch: 0012 | grad_norm: 1.84905326 | loss: 0.5192 | mse: 0.5342\n",
      "epoch: 0013 | grad_norm: 2.86366391 | loss: 0.4894 | mse: 0.5027\n",
      "epoch: 0014 | grad_norm: 2.13982749 | loss: 0.4890 | mse: 0.5010\n",
      "epoch: 0015 | grad_norm: 1.70090890 | loss: 0.4355 | mse: 0.4516\n",
      "epoch: 0016 | grad_norm: 0.44800100 | loss: 0.4391 | mse: 0.4588\n",
      "epoch: 0017 | grad_norm: 1.56249893 | loss: 0.4513 | mse: 0.4707\n",
      "epoch: 0018 | grad_norm: 0.70761949 | loss: 0.4430 | mse: 0.4573\n",
      "epoch: 0019 | grad_norm: 2.20231199 | loss: 0.4002 | mse: 0.4145\n",
      "epoch: 0020 | grad_norm: 1.37999237 | loss: 0.3869 | mse: 0.4051\n",
      "epoch: 0021 | grad_norm: 1.62913835 | loss: 0.3734 | mse: 0.3929\n",
      "epoch: 0022 | grad_norm: 0.95657456 | loss: 0.3429 | mse: 0.3567\n",
      "epoch: 0023 | grad_norm: 1.24451828 | loss: 0.3339 | mse: 0.3437\n",
      "epoch: 0024 | grad_norm: 1.31284404 | loss: 0.3250 | mse: 0.3378\n",
      "epoch: 0025 | grad_norm: 1.33214712 | loss: 0.3193 | mse: 0.3321\n",
      "epoch: 0025 | mse: 0.6361\n",
      "epoch: 0026 | grad_norm: 0.41960454 | loss: 0.3480 | mse: 0.3584\n",
      "epoch: 0027 | grad_norm: 0.61314642 | loss: 0.2848 | mse: 0.2966\n",
      "epoch: 0028 | grad_norm: 0.63417107 | loss: 0.2657 | mse: 0.2775\n",
      "epoch: 0029 | grad_norm: 1.33264589 | loss: 0.2619 | mse: 0.2726\n",
      "epoch: 0030 | grad_norm: 0.83156425 | loss: 0.2581 | mse: 0.2694\n",
      "epoch: 0031 | grad_norm: 0.72786200 | loss: 0.2607 | mse: 0.2673\n",
      "epoch: 0032 | grad_norm: 1.36364508 | loss: 0.2558 | mse: 0.2653\n",
      "epoch: 0033 | grad_norm: 0.54170358 | loss: 0.2520 | mse: 0.2631\n",
      "epoch: 0034 | grad_norm: 1.15150309 | loss: 0.2503 | mse: 0.2607\n",
      "epoch: 0035 | grad_norm: 1.36318576 | loss: 0.2486 | mse: 0.2588\n",
      "epoch: 0036 | grad_norm: 0.89517367 | loss: 0.2471 | mse: 0.2566\n",
      "epoch: 0037 | grad_norm: 0.95181412 | loss: 0.2453 | mse: 0.2545\n",
      "epoch: 0038 | grad_norm: 0.66451174 | loss: 0.2428 | mse: 0.2531\n",
      "epoch: 0039 | grad_norm: 0.30998561 | loss: 0.2425 | mse: 0.2513\n",
      "epoch: 0040 | grad_norm: 1.13514161 | loss: 0.2412 | mse: 0.2499\n",
      "epoch: 0041 | grad_norm: 1.33258581 | loss: 0.2376 | mse: 0.2477\n",
      "epoch: 0042 | grad_norm: 0.26248461 | loss: 0.2352 | mse: 0.2453\n",
      "epoch: 0043 | grad_norm: 0.31228665 | loss: 0.2376 | mse: 0.2442\n",
      "epoch: 0044 | grad_norm: 1.65732110 | loss: 0.2340 | mse: 0.2436\n",
      "epoch: 0045 | grad_norm: 0.60094023 | loss: 0.2302 | mse: 0.2400\n",
      "epoch: 0046 | grad_norm: 1.29030406 | loss: 0.2306 | mse: 0.2391\n",
      "epoch: 0047 | grad_norm: 0.37511620 | loss: 0.2312 | mse: 0.2395\n",
      "epoch: 0048 | grad_norm: 0.50483590 | loss: 0.2296 | mse: 0.2402\n",
      "epoch: 0049 | grad_norm: 1.13899136 | loss: 0.2274 | mse: 0.2361\n",
      "epoch: 0050 | grad_norm: 0.29978442 | loss: 0.2245 | mse: 0.2335\n",
      "epoch: 0050 | mse: 0.3361\n",
      "epoch: 0051 | grad_norm: 0.69649076 | loss: 0.2256 | mse: 0.2364\n",
      "epoch: 0052 | grad_norm: 0.39441687 | loss: 0.2167 | mse: 0.2257\n",
      "epoch: 0053 | grad_norm: 1.04113317 | loss: 0.2156 | mse: 0.2245\n",
      "epoch: 0054 | grad_norm: 0.16974580 | loss: 0.2168 | mse: 0.2241\n",
      "epoch: 0055 | grad_norm: 0.25529146 | loss: 0.2161 | mse: 0.2237\n",
      "epoch: 0056 | grad_norm: 0.39265153 | loss: 0.2144 | mse: 0.2235\n",
      "epoch: 0057 | grad_norm: 0.59761637 | loss: 0.2150 | mse: 0.2232\n",
      "epoch: 0058 | grad_norm: 0.61001438 | loss: 0.2149 | mse: 0.2230\n",
      "epoch: 0059 | grad_norm: 1.31552780 | loss: 0.2142 | mse: 0.2227\n",
      "epoch: 0060 | grad_norm: 0.47984380 | loss: 0.2165 | mse: 0.2226\n",
      "epoch: 0061 | grad_norm: 0.50174272 | loss: 0.2139 | mse: 0.2223\n",
      "epoch: 0062 | grad_norm: 0.46492550 | loss: 0.2132 | mse: 0.2221\n",
      "epoch: 0063 | grad_norm: 0.34416980 | loss: 0.2136 | mse: 0.2218\n",
      "epoch: 0064 | grad_norm: 0.43603933 | loss: 0.2127 | mse: 0.2216\n",
      "epoch: 0065 | grad_norm: 0.94173115 | loss: 0.2131 | mse: 0.2214\n",
      "epoch: 0066 | grad_norm: 0.64397329 | loss: 0.2133 | mse: 0.2212\n",
      "epoch: 0067 | grad_norm: 0.61412770 | loss: 0.2121 | mse: 0.2209\n",
      "epoch: 0068 | grad_norm: 0.84483266 | loss: 0.2129 | mse: 0.2207\n",
      "epoch: 0069 | grad_norm: 0.89418280 | loss: 0.2119 | mse: 0.2204\n",
      "epoch: 0070 | grad_norm: 1.87160456 | loss: 0.2131 | mse: 0.2202\n",
      "epoch: 0071 | grad_norm: 0.59551436 | loss: 0.2117 | mse: 0.2199\n",
      "epoch: 0072 | grad_norm: 0.36855468 | loss: 0.2118 | mse: 0.2197\n",
      "epoch: 0073 | grad_norm: 0.62829351 | loss: 0.2112 | mse: 0.2195\n",
      "epoch: 0074 | grad_norm: 1.11813760 | loss: 0.2107 | mse: 0.2193\n",
      "epoch: 0075 | grad_norm: 0.71477211 | loss: 0.2122 | mse: 0.2190\n",
      "epoch: 0075 | mse: 0.3173\n",
      "epoch: 0076 | grad_norm: 0.52264196 | loss: 0.2114 | mse: 0.2187\n",
      "epoch: 0077 | grad_norm: 0.23404908 | loss: 0.2111 | mse: 0.2186\n",
      "epoch: 0078 | grad_norm: 0.76335967 | loss: 0.2103 | mse: 0.2185\n",
      "epoch: 0079 | grad_norm: 1.36192203 | loss: 0.2101 | mse: 0.2185\n",
      "epoch: 0080 | grad_norm: 0.93248063 | loss: 0.2101 | mse: 0.2185\n",
      "epoch: 0081 | grad_norm: 0.89722043 | loss: 0.2113 | mse: 0.2184\n",
      "epoch: 0082 | grad_norm: 0.64288211 | loss: 0.2112 | mse: 0.2184\n",
      "epoch: 0083 | grad_norm: 0.39780515 | loss: 0.2108 | mse: 0.2184\n",
      "epoch: 0084 | grad_norm: 0.83289903 | loss: 0.2103 | mse: 0.2184\n",
      "epoch: 0085 | grad_norm: 0.96453279 | loss: 0.2102 | mse: 0.2183\n",
      "epoch: 0086 | grad_norm: 0.31855208 | loss: 0.2104 | mse: 0.2183\n",
      "epoch: 0087 | grad_norm: 0.63777775 | loss: 0.2165 | mse: 0.2183\n",
      "epoch: 0088 | grad_norm: 1.77812147 | loss: 0.2106 | mse: 0.2183\n",
      "epoch: 0089 | grad_norm: 0.56974620 | loss: 0.2096 | mse: 0.2182\n",
      "epoch: 0090 | grad_norm: 0.54790878 | loss: 0.2096 | mse: 0.2182\n",
      "epoch: 0091 | grad_norm: 0.25383672 | loss: 0.2121 | mse: 0.2182\n",
      "epoch: 0092 | grad_norm: 0.68812448 | loss: 0.2099 | mse: 0.2182\n",
      "epoch: 0093 | grad_norm: 0.65130049 | loss: 0.2110 | mse: 0.2181\n",
      "epoch: 0094 | grad_norm: 0.65569639 | loss: 0.2126 | mse: 0.2181\n",
      "epoch: 0095 | grad_norm: 0.95700359 | loss: 0.2091 | mse: 0.2181\n",
      "epoch: 0096 | grad_norm: 0.67884094 | loss: 0.2088 | mse: 0.2181\n",
      "epoch: 0097 | grad_norm: 0.46109545 | loss: 0.2098 | mse: 0.2180\n",
      "epoch: 0098 | grad_norm: 0.66367996 | loss: 0.2101 | mse: 0.2180\n",
      "epoch: 0099 | grad_norm: 1.46043479 | loss: 0.2121 | mse: 0.2180\n",
      "epoch: 0100 | grad_norm: 0.71928620 | loss: 0.2093 | mse: 0.2180\n",
      "epoch: 0100 | mse: 0.3163\n",
      "epoch: 0101 | grad_norm: 1.15455174 | loss: 0.2112 | mse: 0.2179\n",
      "epoch: 0102 | grad_norm: 0.53530073 | loss: 0.2102 | mse: 0.2179\n",
      "epoch: 0103 | grad_norm: 0.24815740 | loss: 0.2108 | mse: 0.2179\n",
      "epoch: 0104 | grad_norm: 0.94926608 | loss: 0.2115 | mse: 0.2179\n",
      "epoch: 0105 | grad_norm: 0.50708556 | loss: 0.2115 | mse: 0.2179\n",
      "epoch: 0106 | grad_norm: 1.48880696 | loss: 0.2099 | mse: 0.2179\n",
      "epoch: 0107 | grad_norm: 0.46203989 | loss: 0.2090 | mse: 0.2179\n",
      "epoch: 0108 | grad_norm: 0.31330332 | loss: 0.2096 | mse: 0.2179\n",
      "epoch: 0109 | grad_norm: 0.61811370 | loss: 0.2125 | mse: 0.2179\n",
      "epoch: 0110 | grad_norm: 0.22886962 | loss: 0.2094 | mse: 0.2179\n",
      "epoch: 0111 | grad_norm: 0.33683175 | loss: 0.2110 | mse: 0.2179\n",
      "epoch: 0112 | grad_norm: 0.52352488 | loss: 0.2112 | mse: 0.2179\n",
      "epoch: 0113 | grad_norm: 0.21664041 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0114 | grad_norm: 0.88741785 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0115 | grad_norm: 1.23902869 | loss: 0.2115 | mse: 0.2179\n",
      "epoch: 0116 | grad_norm: 1.08341408 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0117 | grad_norm: 0.66578954 | loss: 0.2102 | mse: 0.2179\n",
      "epoch: 0118 | grad_norm: 0.57600427 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0119 | grad_norm: 0.58676285 | loss: 0.2084 | mse: 0.2179\n",
      "epoch: 0120 | grad_norm: 0.44665188 | loss: 0.2132 | mse: 0.2179\n",
      "epoch: 0121 | grad_norm: 0.52313799 | loss: 0.2097 | mse: 0.2179\n",
      "epoch: 0122 | grad_norm: 0.28857395 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0123 | grad_norm: 0.55581588 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0124 | grad_norm: 0.40400931 | loss: 0.2106 | mse: 0.2179\n",
      "epoch: 0125 | grad_norm: 0.62475491 | loss: 0.2106 | mse: 0.2179\n",
      "epoch: 0125 | mse: 0.3163\n",
      "epoch: 0126 | grad_norm: 0.83191735 | loss: 0.2098 | mse: 0.2179\n",
      "epoch: 0127 | grad_norm: 0.41675287 | loss: 0.2094 | mse: 0.2179\n",
      "epoch: 0128 | grad_norm: 0.37446055 | loss: 0.2096 | mse: 0.2179\n",
      "epoch: 0129 | grad_norm: 0.34465209 | loss: 0.2112 | mse: 0.2179\n",
      "epoch: 0130 | grad_norm: 0.34149337 | loss: 0.2109 | mse: 0.2179\n",
      "epoch: 0131 | grad_norm: 1.22479856 | loss: 0.2094 | mse: 0.2179\n",
      "epoch: 0132 | grad_norm: 0.57706630 | loss: 0.2087 | mse: 0.2179\n",
      "epoch: 0133 | grad_norm: 0.24965245 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0134 | grad_norm: 0.81524098 | loss: 0.2092 | mse: 0.2179\n",
      "epoch: 0135 | grad_norm: 0.66135478 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0136 | grad_norm: 0.68381643 | loss: 0.2104 | mse: 0.2179\n",
      "epoch: 0137 | grad_norm: 0.61737406 | loss: 0.2110 | mse: 0.2179\n",
      "epoch: 0138 | grad_norm: 0.75428188 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0139 | grad_norm: 0.54322493 | loss: 0.2102 | mse: 0.2179\n",
      "epoch: 0140 | grad_norm: 1.05750620 | loss: 0.2098 | mse: 0.2179\n",
      "epoch: 0141 | grad_norm: 0.43839407 | loss: 0.2095 | mse: 0.2179\n",
      "epoch: 0142 | grad_norm: 0.84647918 | loss: 0.2099 | mse: 0.2179\n",
      "epoch: 0143 | grad_norm: 0.94227612 | loss: 0.2118 | mse: 0.2179\n",
      "epoch: 0144 | grad_norm: 1.09234536 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0145 | grad_norm: 0.30456132 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0146 | grad_norm: 0.28924969 | loss: 0.2092 | mse: 0.2179\n",
      "epoch: 0147 | grad_norm: 1.02557576 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0148 | grad_norm: 0.66494036 | loss: 0.2090 | mse: 0.2179\n",
      "epoch: 0149 | grad_norm: 0.86937827 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0150 | grad_norm: 0.29507929 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0150 | mse: 0.3163\n",
      "epoch: 0151 | grad_norm: 0.48528054 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0152 | grad_norm: 0.48478565 | loss: 0.2096 | mse: 0.2179\n",
      "epoch: 0153 | grad_norm: 1.15098751 | loss: 0.2092 | mse: 0.2179\n",
      "epoch: 0154 | grad_norm: 0.89871186 | loss: 0.2102 | mse: 0.2179\n",
      "epoch: 0155 | grad_norm: 0.87408608 | loss: 0.2106 | mse: 0.2179\n",
      "epoch: 0156 | grad_norm: 1.66980171 | loss: 0.2109 | mse: 0.2179\n",
      "epoch: 0157 | grad_norm: 0.30423057 | loss: 0.2107 | mse: 0.2179\n",
      "epoch: 0158 | grad_norm: 1.10829616 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0159 | grad_norm: 0.98059940 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0160 | grad_norm: 1.69999981 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0161 | grad_norm: 0.57819343 | loss: 0.2096 | mse: 0.2179\n",
      "epoch: 0162 | grad_norm: 0.49573475 | loss: 0.2095 | mse: 0.2179\n",
      "epoch: 0163 | grad_norm: 0.56971109 | loss: 0.2090 | mse: 0.2179\n",
      "epoch: 0164 | grad_norm: 0.95290953 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0165 | grad_norm: 0.32592255 | loss: 0.2098 | mse: 0.2179\n",
      "epoch: 0166 | grad_norm: 0.28080031 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0167 | grad_norm: 0.65795588 | loss: 0.2111 | mse: 0.2179\n",
      "epoch: 0168 | grad_norm: 1.11223519 | loss: 0.2109 | mse: 0.2179\n",
      "epoch: 0169 | grad_norm: 0.27679273 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0170 | grad_norm: 0.70186961 | loss: 0.2104 | mse: 0.2179\n",
      "epoch: 0171 | grad_norm: 0.89303666 | loss: 0.2109 | mse: 0.2179\n",
      "epoch: 0172 | grad_norm: 0.87794417 | loss: 0.2088 | mse: 0.2179\n",
      "epoch: 0173 | grad_norm: 0.43927521 | loss: 0.2110 | mse: 0.2179\n",
      "epoch: 0174 | grad_norm: 1.05385458 | loss: 0.2096 | mse: 0.2179\n",
      "epoch: 0175 | grad_norm: 0.26320833 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0175 | mse: 0.3163\n",
      "epoch: 0176 | grad_norm: 0.81686360 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0177 | grad_norm: 0.32215327 | loss: 0.2107 | mse: 0.2179\n",
      "epoch: 0178 | grad_norm: 0.42762679 | loss: 0.2135 | mse: 0.2179\n",
      "epoch: 0179 | grad_norm: 0.90100175 | loss: 0.2102 | mse: 0.2179\n",
      "epoch: 0180 | grad_norm: 0.37881115 | loss: 0.2099 | mse: 0.2179\n",
      "epoch: 0181 | grad_norm: 0.43777278 | loss: 0.2090 | mse: 0.2179\n",
      "epoch: 0182 | grad_norm: 0.70699799 | loss: 0.2110 | mse: 0.2179\n",
      "epoch: 0183 | grad_norm: 0.42167866 | loss: 0.2097 | mse: 0.2179\n",
      "epoch: 0184 | grad_norm: 0.95628357 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0185 | grad_norm: 0.48641157 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0186 | grad_norm: 1.09800661 | loss: 0.2093 | mse: 0.2179\n",
      "epoch: 0187 | grad_norm: 0.41871879 | loss: 0.2103 | mse: 0.2179\n",
      "epoch: 0188 | grad_norm: 0.86729664 | loss: 0.2094 | mse: 0.2179\n",
      "epoch: 0189 | grad_norm: 1.29212081 | loss: 0.2104 | mse: 0.2179\n",
      "epoch: 0190 | grad_norm: 0.38466969 | loss: 0.2089 | mse: 0.2179\n",
      "epoch: 0191 | grad_norm: 1.02628052 | loss: 0.2098 | mse: 0.2179\n",
      "epoch: 0192 | grad_norm: 0.47723666 | loss: 0.2099 | mse: 0.2179\n",
      "epoch: 0193 | grad_norm: 0.35802117 | loss: 0.2100 | mse: 0.2179\n",
      "epoch: 0194 | grad_norm: 0.59362090 | loss: 0.2114 | mse: 0.2179\n",
      "epoch: 0195 | grad_norm: 0.70361739 | loss: 0.2119 | mse: 0.2179\n",
      "epoch: 0196 | grad_norm: 0.80032015 | loss: 0.2088 | mse: 0.2179\n",
      "epoch: 0197 | grad_norm: 0.35041186 | loss: 0.2105 | mse: 0.2179\n",
      "epoch: 0198 | grad_norm: 1.04393280 | loss: 0.2113 | mse: 0.2179\n",
      "epoch: 0199 | grad_norm: 0.24442439 | loss: 0.2101 | mse: 0.2179\n",
      "epoch: 0200 | grad_norm: 0.39973488 | loss: 0.2090 | mse: 0.2179\n",
      "epoch: 0200 | mse: 0.3163\n"
     ]
    }
   ],
   "source": [
    "experiment = \"gbyk\"\n",
    "sessions = ml.sessions.GBYK\n",
    "config = ml.config.load_default(experiment, sessions)\n",
    "\n",
    "config[\"model\"][\"n_layers\"] = 2\n",
    "config[\"poses\"][\"representation\"] = \"centered\"\n",
    "config[\"poses\"][\"keypoints_to_exclude\"] = [\n",
    "    \"e_tail\",\n",
    "    \"s_tail\",\n",
    "    \"l_hip\",\n",
    "    \"l_knee\",\n",
    "    \"l_ankle\",\n",
    "    \"r_hip\",\n",
    "    \"r_knee\",\n",
    "    \"r_ankle\",\n",
    "    \"l_shoulder\",\n",
    "    \"l_elbow\",\n",
    "    \"l_wrist\",\n",
    "    \"r_shoulder\",\n",
    "    \"r_elbow\",\n",
    "    \"r_wrist\",\n",
    "]\n",
    "# config[\"poses\"][\"project_to_pca\"] = True\n",
    "# config[\"poses\"][\"pcs_to_exclude\"] = draconian\n",
    "\n",
    "ml.model.train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ec7d0",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09535edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(f\"config/poses_to_position/{run}.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# config[\"include_trial\"] = True\n",
    "# config[\"include_homing\"] = False\n",
    "\n",
    "# config[\"include_trial\"] = False\n",
    "# config[\"include_homing\"] = True\n",
    "\n",
    "eval_metrics, eval_gts, eval_preds = ml.model.evaluate(config)\n",
    "\n",
    "eval_gts = {session: gt.reshape(-1, 2) for session, gt in eval_gts.items()}\n",
    "tiled_gts = {\n",
    "    session: ml.room.get_tiles(gt[:, 0], gt[:, 1])\n",
    "    for session, gt in eval_gts.items()\n",
    "}\n",
    "\n",
    "eval_preds = {\n",
    "    session: pred.reshape(-1, 2) for session, pred in eval_preds.items()\n",
    "}\n",
    "tiled_preds = {\n",
    "    session: ml.room.get_tiles(pred[:, 0], pred[:, 1])\n",
    "    for session, pred in eval_preds.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f999ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.plot.confusion_matrix(\n",
    "    tiled_gts,\n",
    "    tiled_preds,\n",
    "    group=\"y\",\n",
    "    include_sitting=False,\n",
    "    # concat=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bex_gts = {\n",
    "    session: gts for session, gts in eval_gts.items() if \"bex\" in session\n",
    "}\n",
    "\n",
    "bex_preds = {\n",
    "    session: preds for session, preds in eval_preds.items() if \"bex\" in session\n",
    "}\n",
    "\n",
    "ml.plot.room_histogram2d(bex_gts, bex_preds, concat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bex_tiled_gts = {\n",
    "    session: gt for session, gt in tiled_gts.items() if \"bex\" in session\n",
    "}\n",
    "\n",
    "bex_tiled_preds = {\n",
    "    session: pred for session, pred in tiled_preds.items() if \"bex\" in session\n",
    "}\n",
    "\n",
    "ml.plot.confusion_matrix(\n",
    "    bex_tiled_gts,\n",
    "    bex_tiled_preds,\n",
    "    group=\"y\",\n",
    "    include_sitting=False,\n",
    "    concat=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6394b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ken_gts = {\n",
    "    session: gts for session, gts in eval_gts.items() if \"ken\" in session\n",
    "}\n",
    "\n",
    "ken_preds = {\n",
    "    session: preds for session, preds in eval_preds.items() if \"ken\" in session\n",
    "}\n",
    "\n",
    "ml.plot.room_histogram2d(ken_gts, ken_preds, concat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d378aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ken_tiled_gts = {\n",
    "    session: gt for session, gt in tiled_gts.items() if \"ken\" in session\n",
    "}\n",
    "\n",
    "ken_tiled_preds = {\n",
    "    session: pred for session, pred in tiled_preds.items() if \"ken\" in session\n",
    "}\n",
    "\n",
    "ml.plot.confusion_matrix(\n",
    "    ken_tiled_gts,\n",
    "    ken_tiled_preds,\n",
    "    group=\"y\",\n",
    "    include_sitting=False,\n",
    "    concat=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bex_ken_gts = {\n",
    "    \"bex\": np.concatenate(list(bex_gts.values()), axis=0),\n",
    "    \"ken\": np.concatenate(list(ken_gts.values()), axis=0),\n",
    "}\n",
    "\n",
    "bex_ken_preds = {\n",
    "    \"bex\": np.concatenate(list(bex_preds.values()), axis=0),\n",
    "    \"ken\": np.concatenate(list(ken_preds.values()), axis=0),\n",
    "}\n",
    "\n",
    "ml.plot.room_histogram2d(\n",
    "    bex_ken_gts,\n",
    "    bex_ken_preds,\n",
    "    # save_path=\"plots/pose_to_position/histogram_trial_fc.svg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "bex_ken_tiled_gts = {\n",
    "    \"bex\": np.concatenate(list(bex_tiled_gts.values()), axis=0),\n",
    "    \"ken\": np.concatenate(list(ken_tiled_gts.values()), axis=0),\n",
    "}\n",
    "\n",
    "bex_ken_tiled_preds = {\n",
    "    \"bex\": np.concatenate(list(bex_tiled_preds.values()), axis=0),\n",
    "    \"ken\": np.concatenate(list(ken_tiled_preds.values()), axis=0),\n",
    "}\n",
    "\n",
    "ml.plot.confusion_matrix(\n",
    "    bex_ken_tiled_gts,\n",
    "    bex_ken_tiled_preds,\n",
    "    group=\"y\",\n",
    "    # include_sitting=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 0.865\n",
    "\n",
    "gts = {\n",
    "    \"ideal\": np.array(\n",
    "        [\n",
    "            [1 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "preds = {\n",
    "    \"ideal\": np.array(\n",
    "        [\n",
    "            [3 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 0 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 3 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 2 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 1 * tile_size + tile_size / 2],\n",
    "            [3 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "            [2 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "            [1 * tile_size, 4 * tile_size + tile_size / 2],\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "ml.plot.room_histogram2d(\n",
    "    gts,\n",
    "    preds,\n",
    "    save_path=\"plots/pose_to_position/histogram_homing_ideal.svg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.plot.confusion_matrix(\n",
    "    eval_gts,\n",
    "    eval_preds,\n",
    "    # group=\"x\",\n",
    "    include_sitting=True,\n",
    "    # save_path=\"plots/pose_to_position/confusion_matrix_nofilter_homing.svg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.room.plot(save_path=\"plots/pose_to_position/room.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlab (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
